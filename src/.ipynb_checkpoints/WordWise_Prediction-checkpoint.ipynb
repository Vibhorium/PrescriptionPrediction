{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "import editdistance\n",
    "from DataLoader import DataLoader, Batch\n",
    "from Model import Model, DecoderType\n",
    "from SamplePreprocessor import preprocess\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from WordSegmentation import wordSegmentation, prepareImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediciting words line-by-line\n",
    "imgFiles = os.listdir('../data2/')\n",
    "segmented_files = os.listdir('../out/')\n",
    "class FilePaths:\n",
    "    \"filenames and paths to data\"\n",
    "    fnCharList = '../model/charList.txt'\n",
    "    fnAccuracy = '../model/accuracy.txt'\n",
    "    fnTrain = '../data/'\n",
    "    fnInfer = '../data/4.png'\n",
    "    fnCorpus = '../data/corpus.txt'\n",
    "\n",
    "\n",
    "def train(model, loader):\n",
    "    \"train NN\"\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    bestCharErrorRate = float('inf')  # best valdiation character error rate\n",
    "    noImprovementSince = 0  # number of epochs no improvement of character error rate occured\n",
    "    earlyStopping = 5  # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.trainSet()\n",
    "        while loader.hasNext():\n",
    "            iterInfo = loader.getIteratorInfo()\n",
    "            batch = loader.getNext()\n",
    "            loss = model.trainBatch(batch)\n",
    "            print('Batch:', iterInfo[0], '/', iterInfo[1], 'Loss:', loss)\n",
    "\n",
    "        # validate\n",
    "        charErrorRate = validate(model, loader)\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if charErrorRate < bestCharErrorRate:\n",
    "            print('Character error rate improved, save model')\n",
    "            bestCharErrorRate = charErrorRate\n",
    "            noImprovementSince = 0\n",
    "            model.save()\n",
    "            open(FilePaths.fnAccuracy, 'w').write(\n",
    "                'Validation character error rate of saved model: %f%%' % (charErrorRate * 100.0))\n",
    "        else:\n",
    "            print('Character error rate not improved')\n",
    "            noImprovementSince += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if noImprovementSince >= earlyStopping:\n",
    "            print('No more improvement since %d epochs. Training stopped.' % earlyStopping)\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    \"validate NN\"\n",
    "    print('Validate NN')\n",
    "    loader.validationSet()\n",
    "    numCharErr = 0\n",
    "    numCharTotal = 0\n",
    "    numWordOK = 0\n",
    "    numWordTotal = 0\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        print('Batch:', iterInfo[0], '/', iterInfo[1])\n",
    "        batch = loader.getNext()\n",
    "        (recognized, _) = model.inferBatch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
    "            numWordTotal += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
    "            numCharErr += dist\n",
    "            numCharTotal += len(batch.gtTexts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gtTexts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    charErrorRate = numCharErr / numCharTotal\n",
    "    wordAccuracy = numWordOK / numWordTotal\n",
    "    print('Character error rate: %f%%. Word accuracy: %f%%.' % (charErrorRate * 100.0, wordAccuracy * 100.0))\n",
    "    return charErrorRate\n",
    "\n",
    "\n",
    "def infer(model, fnImg):\n",
    "    \"recognize text in image provided by file path\"\n",
    "    print(fnImg)\n",
    "    img = preprocess(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), Model.imgSize)\n",
    "    batch = Batch(None, [img])\n",
    "    (recognized, probability) = model.inferBatch(batch, True)\n",
    "    print('Recognized:', '\"' + recognized[0] + '\"')\n",
    "    print('Probability:', probability[0])\n",
    "    return recognized[0]\n",
    "\n",
    "\n",
    "def wpredict():\n",
    "    decoderType = DecoderType.BestPath\n",
    "    \"main function\"\n",
    "    # optional command line args\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', help='train the NN', action='store_true')\n",
    "    parser.add_argument('--validate', help='validate the NN', action='store_true')\n",
    "    parser.add_argument('--beamsearch', help='use beam search instead of best path decoding', action='store_true')\n",
    "    parser.add_argument('--wordbeamsearch', help='use word beam search instead of best path decoding',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--dump', help='dump output of NN to CSV file(s)', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    decoderType = DecoderType.BestPath\n",
    "    if args.beamsearch:\n",
    "        decoderType = DecoderType.BeamSearch\n",
    "    elif args.wordbeamsearch:\n",
    "        decoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "    # train or validate on IAM dataset\n",
    "    if args.train or args.validate:\n",
    "        # load training data, create TF model\n",
    "        loader = DataLoader(FilePaths.fnTrain, Model.batchSize, Model.imgSize, Model.maxTextLen)\n",
    "\n",
    "        # save characters of model for inference mode\n",
    "        open(FilePaths.fnCharList, 'w').write(str().join(loader.charList))\n",
    "\n",
    "        # save words contained in dataset into file\n",
    "        open(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "        # execute training or validation\n",
    "        if args.train:\n",
    "            model = Model(loader.charList, decoderType)\n",
    "            train(model, loader)\n",
    "        elif args.validate:\n",
    "            model = Model(loader.charList, decoderType, mustRestore=True)\n",
    "            validate(model, loader)\n",
    "\n",
    "    # infer text on test image\n",
    "    else:\n",
    "        print(open(FilePaths.fnAccuracy).read())\n",
    "        model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\n",
    "        for (i, f) in enumerate(segmented_files):\n",
    "            print(f)\n",
    "            imgFolder = os.listdir('../out/' + f + '/')\n",
    "            imgPath = '../out/' + f + '/'\n",
    "            s=\"\"\n",
    "            for j in range(0, len(imgFolder) - 1):\n",
    "                print(imgFolder[j])\n",
    "                s=s+infer(model, imgPath + str(imgFolder[j]))\n",
    "                s=s+\" \"\n",
    "            linewise_strings.append(s)\n",
    "        \n",
    "        for i in range(0, len(imgFiles)):\n",
    "            print(imgFiles[i])\n",
    "            imgFolder = os.listdir('../out/' + str(imgFiles[i]) + '/')\n",
    "            imgPath = '../out/' + imgFiles[i] + '/'\n",
    "            for j in range(0, len(imgFolder) - 1):\n",
    "                print(imgFolder[j])\n",
    "                infer(model, imgPath + str(imgFolder[j]))\n",
    "    \"\"\"\n",
    "def word_pred():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', help='train the NN', action='store_true')\n",
    "    parser.add_argument('--validate', help='validate the NN', action='store_true')\n",
    "    parser.add_argument('--beamsearch', help='use beam search instead of best path decoding', action='store_true')\n",
    "    parser.add_argument('--wordbeamsearch', help='use word beam search instead of best path decoding',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--dump', help='dump output of NN to CSV file(s)', action='store_true')\n",
    "    parser.add_argument('-f',action=\"store_true\")\n",
    "    #parser.add_argument('-h',action=\"store_true\")\n",
    "    parser.add_argument('C:\"\\\"Users\"\\\"Kartik\"\\\"AppData\"\\\"Roaming\"\\\"jupyter\"\\\"runtime\"\\\"kernel-1fe87222-15ed-456e-bf5a-36632182a329.json')\n",
    "    args = parser.parse_args()\n",
    "    args.beamsearch=False\n",
    "    args.wordbeamsearch=False\n",
    "    args.dump=False\n",
    "    args.train=False\n",
    "    args.validate=False\n",
    "    decoderType = DecoderType.BestPath\n",
    "    print(open(FilePaths.fnAccuracy).read())\n",
    "    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True,dump=False)\n",
    "    for (i, f) in enumerate(segmented_files):\n",
    "        print(f)\n",
    "        imgFolder = os.listdir('../out/' + f + '/')\n",
    "        imgPath = '../out/' + f + '/'\n",
    "        s=\"\"\n",
    "        for j in range(0, len(imgFolder) - 1):\n",
    "            print(imgFolder[j])\n",
    "            s=s+infer(model, imgPath + str(imgFolder[j]))\n",
    "            s=s+\" \"\n",
    "        linewise_strings.append(s)\n",
    "    for i in range(0, len(imgFiles)):\n",
    "                print(imgFiles[i])\n",
    "                imgFolder = os.listdir('../out/' + str(imgFiles[i]) + '/')\n",
    "                imgPath = '../out/' + imgFiles[i] + '/'\n",
    "                for j in range(0, len(imgFolder) - 1):\n",
    "                    print(imgFolder[j])\n",
    "                    infer(model, imgPath + str(imgFolder[j]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
